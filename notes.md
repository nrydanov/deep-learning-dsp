## Эксперименты

Модели:
1. many-to-many. Выход для $a_k$ — линейное преобразование над $h_k$
2. many-to-one. Выход для $a_k$ — результат свертки и обработки LSTM предыдущих $n$ сэмплов


Выводы:

1. ETS довольно нестабильная для loss-функции. Батч-нормализация + клиппинг
сглаживают ситуацию, однако не до конца понятно, насколько.
2. Оптимальное MSE не обеспечивает оптимальное ETS, более того, хотя
MSE содержится в ETS, их значения могут очень плохо коррелировать друг с другом.
3. Обеспечивая неоптимальное значение ETS, сеть обеспечивает неоптимальное
с точки зрения субъективных ощущений от прослушивания качество звука.
4. Модель many-to-one более информативная, так как делает вывод на основе
некоторого количества предыдущих значений. Однако, она требует бОльших
вычислительных ресурсов.
5. Выбор LR для обучения с MSE в качестве loss-а неочевидный, лучший результат
many-to-one сеть показала с LR=0.01.
